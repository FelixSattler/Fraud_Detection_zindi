{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA: fraud detection\n",
    "### Fraud Detection in Electricity and Gas Consumption Challenge from Zindi\n",
    "### Detect clients commiting fraud in Tunisia regarding their electricity and gas consumption based on their billing history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from eda_tables import explore\n",
    "import eda_viz as ev "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import data first check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "client_raw = pd.read_csv('data/client_train.csv')\n",
    "invoice_raw = pd.read_csv('data/invoice_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first info including missing values and duplicates\n",
    "explore(invoice_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first info including missing values and duplicates\n",
    "explore(client_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming some columns\n",
    "client_raw = client_raw.rename(columns={'disrict': 'district'})\n",
    "\n",
    "invoice_raw = invoice_raw.rename(columns={'consommation_level_1': 'CL_1', 'consommation_level_2': 'CL_2',\n",
    "                                          'consommation_level_3': 'CL_3', 'consommation_level_4': 'CL_4'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client value counts\n",
    "client_cat = client_raw[['district', 'client_catg', 'region', 'target']]\n",
    "ev.categorical_value_counts(client_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of clients per district and region\n",
    "for col in ['district','region']:\n",
    "    region = client_raw.groupby([col])['client_id'].count()\n",
    "    plt.bar(x=region.index, height=region.values)\n",
    "    plt.title('Distribution across ' + col +'s')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of frauds to non frauds\n",
    "fraud = client_raw.groupby(['target'])['client_id'].count()\n",
    "fraud_percentage = (fraud / fraud.sum()) * 100\n",
    "plt.bar(x=fraud.index, height=fraud.values, tick_label=[0, 1])\n",
    "plt.title('Fraud Distribution')\n",
    "\n",
    "for index, value in enumerate(fraud.values):\n",
    "    percentage = fraud_percentage[index]\n",
    "    plt.text(index, value + 10, f'{percentage:.2f}%', ha='center', va='top', fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of frauds per district and region\n",
    "for col in ['district','region']:\n",
    "   \n",
    "    total_clients = client_raw.groupby(col)['client_id'].count()\n",
    "    total_frauds = client_raw.groupby(col)['target'].sum()\n",
    "    relative_frauds = (total_frauds / total_clients) * 100\n",
    "    \n",
    "    #plt.figure(figsize=(10, 5))\n",
    "    plt.bar(relative_frauds.index, relative_frauds.values)\n",
    "    plt.title(f'Relative frauds per {col}')\n",
    "    plt.xlabel(col.capitalize())\n",
    "    plt.ylabel('Percentage of frauds')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoice value counts\n",
    "invoice_cat = invoice_raw[['tarif_type', 'counter_statue', 'counter_code', 'reading_remarque',\n",
    "                           'counter_coefficient','counter_type']]\n",
    "\n",
    "ev.categorical_value_counts(invoice_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLs = invoice_raw[['CL_1','CL_2','CL_3','CL_4']]\n",
    "ev.plot_distributions(CLs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove duplicates, NaNs, outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no NaNs in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "invoice_cleaned = invoice_raw.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove extreme outliers (1% and 99% quantile, instead of e.g. z-score, due to extreme skewness)\n",
    "for col in ['CL_1', 'CL_2', 'CL_3', 'CL_4']:\n",
    "    lower_bound = invoice_cleaned[col].quantile(0.01)\n",
    "    upper_bound = invoice_cleaned[col].quantile(0.99)\n",
    "    invoice_filtered = invoice_cleaned[(invoice_cleaned[col] >= lower_bound) & (invoice_cleaned[col] <= upper_bound)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create additional values\n",
    "CLs = ['CL_1','CL_2','CL_3','CL_4']\n",
    "\n",
    "for CL in CLs: \n",
    "    CL_diff = f'diff_{CL}'\n",
    "    inc_diff = f'inc_{CL}'\n",
    "    rel_diff = f'rel_diff_{CL}'\n",
    "    large_diff = f'large_diff_{CL}'\n",
    "    invoice_filtered[CL] = invoice_filtered[CL].replace(0, 1e-9)  # Replaces 0 with a tiny number\n",
    "\n",
    "    invoice_filtered[CL_diff] = invoice_filtered.groupby('client_id')[CL].diff().fillna(0)  \n",
    "    invoice_filtered[inc_diff] = (invoice_filtered.groupby('client_id')[CL].diff().fillna(0) > 0).astype(int)\n",
    "    invoice_filtered[rel_diff] = invoice_filtered.groupby('client_id')[CL].pct_change().fillna(0) \n",
    "    invoice_filtered[large_diff] = (invoice_filtered.groupby('client_id')[CL].pct_change().fillna(0) > invoice_filtered[CL].quantile(0.75) ).astype(int) \n",
    "\n",
    "print(invoice_filtered.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_or_single(series):\n",
    "    if len(series) == 1:\n",
    "        return series.iloc[0]  # Take the single value if only one invoice\n",
    "    counts = series.value_counts()\n",
    "    if len(counts) > 1 and counts.iloc[0] == counts.iloc[1]:\n",
    "        return 1  # If there's a tie, take True (1)\n",
    "    return counts.idxmax()  # Otherwise, take the most frequent value (mode)\n",
    "\n",
    "# Define custom function to calculate the percentage of 1s \n",
    "def percentage_of_ones(series):\n",
    "    return (series.sum() / len(series)) \n",
    "\n",
    "\n",
    "# Aggregate diff_features\n",
    "diff_features_bool = invoice_filtered.groupby('client_id')[['inc_CL_1','inc_CL_2','inc_CL_3','inc_CL_4',\n",
    "                                                            'large_diff_CL_1','large_diff_CL_2','large_diff_CL_3','large_diff_CL_4'\n",
    "                                                            ]].agg(majority_or_single).reset_index()\n",
    "diff_features_rel = invoice_filtered.groupby('client_id')[['inc_CL_1','inc_CL_2','inc_CL_3','inc_CL_4',\n",
    "                                                           'large_diff_CL_1','large_diff_CL_2','large_diff_CL_3','large_diff_CL_4'\n",
    "                                                           ]].agg(percentage_of_ones).reset_index()\n",
    "# Ensure the values are integers\n",
    "diff_features_bool[['inc_CL_1','inc_CL_2','inc_CL_3','inc_CL_4',\n",
    "               'large_diff_CL_1','large_diff_CL_2','large_diff_CL_3','large_diff_CL_4'\n",
    "               ]] = diff_features_bool[[\n",
    "                   'inc_CL_1','inc_CL_2','inc_CL_3','inc_CL_4',\n",
    "                   'large_diff_CL_1','large_diff_CL_2','large_diff_CL_3','large_diff_CL_4'\n",
    "               ]].astype(int)\n",
    "\n",
    "# Add suffix to every column name except client_id\n",
    "diff_features_rel = diff_features_rel.rename(columns=lambda col_name: col_name if col_name == 'client_id' else col_name + '_rel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by column 'client_id' and calculate the mean of consommation_levels\n",
    "mean_result = invoice_filtered.groupby('client_id')[['CL_1','CL_2','CL_3','CL_4']].mean().reset_index() \n",
    "# Count the number of elements in each group \n",
    "count_result = invoice_filtered.groupby('client_id')[['CL_1','CL_2','CL_3','CL_4']].size().reset_index(name='no_invoices') \n",
    "\n",
    "# Merge the mean and count results \n",
    "merged_invoice_1 = pd.merge(mean_result, count_result,  on='client_id')  \n",
    "merged_invoice_2 = pd.merge(merged_invoice_1 , diff_features_bool,  on='client_id')  \n",
    "merged_invoice = pd.merge(merged_invoice_2, diff_features_rel,  on='client_id')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = client_raw # nothing to clean, filter, etc in client data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge dfs\n",
    "df = client.merge(merged_invoice, how='left', on= 'client_id')\n",
    "\n",
    "df.shape[1] == merged_invoice.shape[1] + client.shape[1]-1 # ncol equal? Yes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create date columns\n",
    "df['creation_date'] = pd.to_datetime(df['creation_date'])\n",
    "df['day'] = df['creation_date'].dt.day.apply(lambda x: f'{x:02d}')\n",
    "df['month'] = df['creation_date'].dt.month.apply(lambda x: f'{x:02d}')\n",
    "df['year'] = df['creation_date'].dt.year\n",
    "df['date_int'] = df[['year', 'month', 'day']].apply(lambda row: ''.join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final check\n",
    "explore(df)\n",
    "# missing values, no of unique client ids in df 'client' > no of unique client ids in df 'merged_invoice'\n",
    "# --> remove rows with na\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df as csv\n",
    "\n",
    "df.to_csv('data/merged_train.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
